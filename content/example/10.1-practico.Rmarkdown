---
title: "Regresiones lineales"
linktitle: "10.1: Regresiones lineales"
date: "2021-08-13"
menu:
  example:
    parent: Ejemplos
    weight: 10
type: docs
toc: true
editor_options: 
  chunk_output_type: console
---

# 0. Objetivo de la práctica

El objetivo del práctico, es avanzar en la etapa de análisis de los datos a través del uso de **regresiones lineales**. Para esto usaremos datos previamente procesados de la base de datos a utilizar. ¡No olvidemos que nos encontramos en el análisis de datos!


![](https://github.com/learn-R/slides/raw/main/img/01/flow-rproject.png)

Entonces, en esta práctica aprenderemos trabajar las __regresiones lineales__, trabajando con __predictores continuos y categóricos__. Luego veremos las diferencias de la estimación simple con **pesos muestrales**, para continuar con la __extracción de objetos__ y finalmente veremos como __representarlos mediante gráficos y tablas__. 


# 1. Recursos del práctico

En este práctico utilizamos los **datos procesados** de la [**Encuesta Suplementaria de Ingresos (ESI) 2020**](https://www.ine.cl/estadisticas/sociales/ingresos-y-gastos/encuesta-suplementaria-de-ingresos).Recuerden _**siempre**_ consultar el [**libro códigos**](https://www.ine.cl/docs/default-source/encuesta-suplementaria-de-ingresos/bbdd/manual-y-gu%C3%ADa-de-variables/2020/personas-esi-2020.pdf?sfvrsn=f196cb4e_4) antes de trabajar datos.

- [<i class="fas fa-file-archive"></i> `10.1-clase.zip`](https://github.com/learn-R/10.1-class/raw/main/10.1-clase.zip)



# 2. Paquetes a utilizar

En este práctico utilizaremos siete paquetes

1. `pacman`: este facilita y agiliza la lectura de los paquetes a utilizar en R;

2. `tidyverse`: colección de paquetes;

3. `srvyr`: para crear el objeto encuesta;

4. `survey`: para crear modelos incorporando el diseño muestral;

5. `sjPlot`: para presentar tablas y gráficos con los modelos creados;


```{r librerias, echo=TRUE, results='hide'}
pacman::p_load(sjPlot, 
               tidyverse, 
               srvyr,
               survey)
```


# 3. Importar datos

Una vez cargado los paquetes a utilizar, debemos cargar los datos procesados.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

datos <- readRDS(url("https://github.com/learn-R/10.1-class/raw/main/input/data/datos_proc.rds"))

```

```{r, eval = F }
datos <- readRDS("output/data/datos_proc.rds")
```

## 4. Explorar datos

Es relevante explorar los datos que utilizaremos, cómo están previamente procesados ¡no sabemos con que variables estamos trabajando!

```{r names, echo=TRUE, results='markup'}
names(datos)
```


```{r head, echo=TRUE, results='markup'}
head(datos)
```

Ahora sabemos que trabajaremos con seis variables: `"ing_t_t"`, `"est_conyugal"`, `"ciuo08"`, `"sexo"`, `"edad"` y `"fact_cal_esi"`,. Ahora inclusive podemos explorar nuestros datos con `sjPlot::view_df()`

```{r echo=TRUE}
sjPlot::view_df(datos,
                encoding = "UTF-8")
```

Perfecto, podemos ver las variables que tenemos y sus categorías de respuesta pasaremos a la **regresión**

# 5. Modelo de regresión lineal

## 5.1 Formula

Previo a eso recordemos que la fórmula de la regresión lineal simple es:

\begin{equation}
\widehat{Y}=b_{0} +b_{1}X
\end{equation}

Mientras que en la regresión lineal múltiple es:

\begin{equation}
\widehat{Y}=b_{0} +b_{1}X +b_{2}X +b_{x}X
\end{equation}

Donde

- $\widehat{Y}$ es el valor estimado/predicho de $Y$
- $b_{0}$ es el **intercepto** de la recta (el valor de Y cuando X es 0)
- $b_{1}$ y $b_{2}$ son los **coeficiente de regresión**, que nos dice cuánto aumenta Y por cada punto que aumenta X (pendiente)

{{< div "note" >}}

### Ecuaciones en LateX

Previo a revisar regresiones en R, es necesario aprender cómo están hechas estas ecuaciones

Hay dos maneras de realizarlo. Para ello será muy importante el signo `$`. Para entenderlo usaremos el ejemplo que está arriba 

Forma 1 de ecuación: Esta forma se utiliza para que la fórmula esté centrada en una linea específica       

```
\begin{equation}
\widehat{Y}=b_{0} +b_{1}X
\end{equation}
```

- Resultado

\begin{equation}
\widehat{Y}=b_{0} +b_{1}X
\end{equation}

- Con 

\begin indicamos el inicio y con las llaves ({}) especificamos que queremos escribir una ecuación {equation}
\widehat  especificamos la ecuación
\end decimos a R que es el fin de la ecuación {equation}


Forma 2: Esta forma se utiliza para que la fórmula este de forma contínua en el texto (_inline_)

```
${Y} = b_{0} +b_{1}X$
```

- Resultado

${Y}=b_{0} +b_{1}X$

Forma 3: Con esta forma la ecuación se verá centrada

```
$${Y}=b_{0} +b_{1}X$$
```

- Resultado 

$${Y}=b_{0} +b_{1}X$$

Si quieres aprender más sobre el uso de ecuaciones en LateX, dirígete [acá](https://manualdelatex.com/tutoriales/ecuaciones)

{{</div>}}

Les mostramos esto porque de la misma forma se diferencian ambos procedimientos en R

Para la regresión lineal simple se utiliza la siguiente estructura:

```
objeto <- lm(dependiente ~ independiente, data=datos)
```

Mientras que para la regresión lineal múltiple, sólo se añaden más variables con el signo **+**

```
objeto <- lm(dependiente ~ independiente1 + independiente 2 + independientex, data=datos)
```

## 5.2 Modelo 

En este práctico no sólo aprenderemos a utilizar regresiones en R, sino que también a utilizalas con pesos muestrales. Pero, ¿cuál es la diferencia? ¡Ahora lo veremos!

### Modelo nulo sin pesos muestrales 

Para ello crearemos en un objeto nuestro modelo nulo

```{r}
modelo0_sin <- lm(ing_t_t ~ 1,
              data = datos)
```
 y lo visualizaremos con 
 
```{r}
summary(modelo0_sin)
```


### Modelo nulo con pesos muestrales

Luego crearemos el modelo con peso muestral, para esto debemos añadir un nuevo argumento `weights`, donde se especifica el factor de expansión 

```{r}
modelo0 <- lm(ing_t_t ~ 1,
              data = datos, 
              weights = fact_cal_esi)
```

Ahora veremos los resultados con 

```{r}
summary(modelo0)
```

Entonces ¿qué diferencias hay?

```{r}
summary(modelo0);summary(modelo0_sin)
```

Utilizando ecuaciones en LateX, se vería de la siguiente manera 

Modelo sin muestral

\begin{equation}
\widehat{Y}= 206355 + b_{1}X
\end{equation}

Modelo con peso muestral

\begin{equation}
\widehat{Y}= 256201 + b_{1}X
\end{equation}

Hay diferencias en el intercepto y en los errores estándar, ya que uno asume pesos de precisión y otro asume pesos muestrales.

Por ello es muy importante incorporar el peso muestral en el análisis, sino los estimadores puntuales no serán exactos, **no podremos realizar inferencia a nivel poblacional a partir del modelo**, debido a la sobrestimación de la precisión de los estimadores. 

## 5.3 Regresión lineal simple 

Ahora estimaremos modelos simples 

El primero sera entre ingresos y edad (`ing_t_t`, `edad`)

```{r modelo1, echo=TRUE, message=FALSE, warning=FALSE, results='markup'}
modelo1 <- lm(ing_t_t ~ edad,
              data = datos, 
              weights = fact_cal_esi)
```

```{r}
summary(modelo1)
```

Ecuación:

```
\begin{equation}
\widehat{Y}= b_{0} +b_{1}X
\end{equation}

Reemplazamos

\begin{equation}
\widehat{ing_t_t}= 127658.05 + 3511.38*edad
\end{equation}
```

Resultado: 

\begin{equation}
\widehat{ing_t_t}= 127658.05 + 3511.38*edad
\end{equation}


También podemos ver la relación entre ingresos y sexo (`ing_t_t`, `sexo`)

```{r modelo2_sin, echo=TRUE, message=FALSE, warning=FALSE, results='markup'}
modelo2_sin <- lm(ing_t_t ~ sexo,
              data = datos, 
              weights = fact_cal_esi)
```

```{r}
summary(modelo2_sin)
```

¡Pero espera! ¡`sexo` no es una variable continua!

### Predictores categóricos

Previo a esto hay que recordar que `sexo` no es un predictor continuo, y también debemos recordárselo a R. **Este es un proceso que debe realizarse en el código de preparación, pero para ver como procesa R los predictores categóricos en una regresión lo haremos acá**

Para ello utilizamos `forcats` un paquete del universo `tidyverse` con la función `as_factor`

```{r}
datos$sexo <-  forcats::as_factor(datos$sexo) #Recuerden que esto debería ir en el código de preparación
```

¡Vamos al modelo!

```{r}
modelo2 <- lm(ing_t_t ~ sexo,
              data = datos, 
              weights = fact_cal_esi)
```

```{r}
summary(modelo2)
```

Como pueden ver, ahora especifica qué categoría de respuesta utiliza como nivel de referencia, esto en la variable sexo quizá puede ser intuitivo, pero ¿qué pasaría con las que tienen otras categorías de respuesta?

intentémoslo con `ciuo08` y `est_conyugal`

```{r}
datos$ciuo08 <-  forcats::as_factor(datos$ciuo08) 
datos$est_conyugal <-  forcats::as_factor(datos$est_conyugal)
# ¡No olviden que esto debe ir en el código de procesamiento!
```

Modelo para `ciuo08`

```{r}
modelo3 <- lm(ing_t_t ~ ciuo08,
              data = datos, 
              weights = fact_cal_esi)

```

```{r}
summary(modelo3)
```

Modelo para `est_conyugal`

```{r}
modelo4 <- lm(ing_t_t ~ est_conyugal,
              data = datos, 
              weights = fact_cal_esi)
```

```{r}
summary(modelo4)
```

En síntesis, el nivel de referencia **siempre será el level más bajo de cada variable**. Por ello es fundamental asegurarnos de que los factores que generemos en el código de procesamiento sea correcto y se adecue a nuestros fines. 

## 5.4  Regresión lineal múltiple

¡Incorporemos más variables con el signo más (+)!

```{r regmult, echo=TRUE, message=FALSE, warning=FALSE, results='markup'}
modelo5 <- lm(ing_t_t ~ edad + sexo + ciuo08 + est_conyugal,
              data = datos, 
              weights = fact_cal_esi)
```

```{r}
summary(modelo5)
```

### Regresión con paquete  

[`stats`](https://www.rdocumentation.org/packages/stats/versions/3.6.2)

`stats` tiene su función para generar diversos tipo de regresiones. Esta función es `glm()`, y tiene una estructura similar a `lm()`, sólo que en esta se debe especificar el tipo de regresión que se necesita en el argumento [`family`](https://www.statmethods.net/advstats/glm.html). Así quedaría nuestro modelo de regresión lineal:

```{r}
modelo5_glm <- glm(ing_t_t ~ edad + sexo + ciuo08 + est_conyugal,
               family = gaussian(link = "identity"), #Especificamos la regresión lineal
               data = datos, 
               weights = fact_cal_esi)
```

```{r}
summary(modelo5_glm)
```

### Regresión con paquete `survey`

`survey` también tiene su función para regresiones, para ello primero crearemos un objeto encuesta

```{r}
esi_design <- as_survey_design(datos, 
                               ids = 1, 
                               weights = fact_cal_esi)
```

Luego usamos la función `svyglm` y en el argumento `design` dejamos el objeto creado y así  generamos nuestro modelo:

```{r}
modelo5_survey <- svyglm(ing_t_t ~ edad + sexo + ciuo08 + est_conyugal,
                         family = gaussian(link = "identity"),
                         design = esi_design)
```

Ahora visualizamos 

```{r}
summary(modelo5_survey)
```

## Modelos de regresión logística

## 5.4 Información de los modelos

Para conocer la información de los modelos, hay múltiples funciones que podemos utilizar:

La primera función es `str`, la cual nos mostrará su estructura

```{r}
str(modelo5)
```

Luego esta `summary`, la cual hemos estado utilizando a lo largo del práctico 

```{r}
summary(modelo5)
```

¿Pero que pasa si queremos un elemento en particular del modelo?, como este es un objeto, se puede utilizar el símbolo `$` para extraer información de este

```{r}
modelo5$coefficients
modelo5$coefficients[2]
modelo5$coefficients["edad"]
```

¿Qué otros elementos puedo extraer?, para eso `str` nos dirá estructura y elementos que podremos utilizar

```{r}
str(summary(modelo5))
```

```{r}
summary(modelo5)$fstatistic
summary(modelo5)$r.squared
```


Para los valores predichos utilizaremos la función de `sjPlot::get_model_data()`


```{r eval=FALSE, include=T}
modelo5$fitted.values
```

```{r}
get_model_data(modelo5, 
               type = "pred")
get_model_data(modelo5, 
               type = "pred", 
               terms = "sexo")
```


{{< div "note" >}}

### BONUS

Con `broom`, podemos convertir objetos en dataframes, lo cual nos permitirá presentarlo con `tab_df` de `sjPlot`

```{r}
print <- broom::augment(modelo5) 
```

```{r eval=FALSE, include=T}
tab_df(print, file = "output/data/modelo.doc")
```
{{</div>}}


Esto nos lleva al último punto del práctico, la visualización 


# 6. Visualización 

Si bien `summary` es un código muy útil para visualizar los modelos creados, el problema es que al observar el objeto creado, no es muy presentable para informes, por eso usaremos la función `tab_model` de `sjPlot`, que tiene la siguiente estructura:

```{r reg-ej, eval=FALSE, message=FALSE, warning=FALSE, include=TRUE}
sjPlot::tab_model(objeto_creado, 
                  show.ci= F/T,  # este argumento muestra los intervalos de confianza
                  show.p = F/T, #Este argumento muestra los valores p
                  show.obs = F/T, # Este argumento muestra las observaciones
                  title = "Título de la tabla a crear",
                  digits = 2, # muestra la cantidad de dígitos que tednrá la tabla
                  p.style = c("numeric", "stars", "numeric_stars", "scientific", "scientific_stars"), #cómo representa el pvalue 
                  encoding = "UTF-8",  # evita errores en caracteres
                  file = "output/figures/reg1_tab.doc") # guarda lo creado automáticamente
```

Para visualizar sólo un modelo puede hacerse de esta forma y en nuestros datos se vería así:

```{r reg-ej-s, echo=TRUE, message=FALSE, warning=FALSE, results='markup'}
sjPlot::tab_model(modelo0, 
                  show.ci=FALSE,  
                  encoding = "UTF-8", 
                  file = "output/figures/regnc_tab.doc")
```

Las ventajas de este código es que se puede especificar el `encoding`, así como exportar la tabla, cuya presentación es superior al output de la función `summary()`

Pero además se pueden incluir más modelos creados en una sola tabla, para eso usaremos nuevamente la función `tab_model` de `sjPlot` 

```{r message=FALSE, warning=FALSE}
sjPlot::tab_model(list(modelo0, modelo1, modelo2), # los modelos estimados
  show.ci=FALSE, # no mostrar intervalo de confianza (por defecto lo hace)
  p.style = "stars", # asteriscos de significación estadística
  dv.labels = c("Modelo 1", "Modelo 2", "Modelo 3"), # etiquetas de modelos o variables dep.
  string.pred = "Predictores", string.est = "β", # nombre predictores y símbolo beta en tabla
  encoding =  "UTF-8",
  file = "output/figures/reg_tab_all.doc")
```

También se pueden especificar las etiquetas con `dv.labels` y cómo queremos que nos incorpore la significación estadística, con  `p.style`


Para visualizar o graficar los coeficientes de regresión para poder observar el impacto de cada variable en el modelo utilizaremos la función `plot_model` de `sjPlot`, su estructura es la siguiente:

```{r eval=FALSE, include=TRUE}
sjPlot::plot_model(objeto_creado, 
                   ci.lvl = "", #estima el nivel de confianza 
                   title = "",  # es el título
                   show.p = T, # nos muestra los valores p
                   show.values =  T, # nos muestra los valores
                   vline.color = "") # color de la recta vertical
                   

```

Esto visualizado con nuestro modelo se ve así:

```{r message=FALSE, warning=FALSE, , echo=TRUE}
sjPlot::plot_model(modelo5, 
                   show.p = T,
                   show.values =  T,
                   ci.lvl = c(0.95), 
                   title = "Estimación de predictores", 
                   vline.color = "purple")
```


Terminamos por este práctico ¡Pero aún falta la regresión logística!, para eso nos vemos en la próxima clase

# 7. Resumen

En este práctico aprendimos a 

- Crear y visualizar regresiones lineales y logísticas binomiales
- Incorporar predictores categóricos
- Crear y visualizar regresiones múltiples

# 8. Recursos

- [sjPlot](https://strengejacke.github.io/sjPlot/) 
- [tidyverse](https://www.tidyverse.org/packages/)
- [magrittr](https://magrittr.tidyverse.org/)
